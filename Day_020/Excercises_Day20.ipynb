{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "200\n",
      "{'Date': 'Wed, 01 Mar 2023 22:05:44 GMT', 'Server': 'Apache', 'Last-Modified': 'Sat, 17 Oct 2020 15:05:23 GMT', 'Accept-Ranges': 'bytes', 'Content-Length': '179410', 'Content-Type': 'text/plain'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The most frequent words are: [('the', 762), ('I', 549), ('and', 539), ('to', 522), ('of', 485), ('a', 453), ('in', 330), ('is', 322), ('my', 310), ('with', 274)]\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read this url and find the 10 most frequent words. romeo_and_juliet\n",
    "\n",
    "# Let's import the requests package and estalish anetwork connection\n",
    "import requests\n",
    "from collections import Counter\n",
    "url= 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "# Opening the network and fetching data\n",
    "response=requests.get(url)\n",
    "# printing response object\n",
    "print (response)\n",
    "# response status\n",
    "print(response.status_code)\n",
    "# header\n",
    "print(response.headers)\n",
    "# fetching the text from the web page\n",
    "txt=response.text\n",
    "\n",
    "# Let's define a function to find the most frequent words\n",
    "def most_frequent(text,k):\n",
    "    words = text.split()\n",
    "    counter=Counter(words)\n",
    "    most_frequent=counter.most_common(k)\n",
    "    return f'The most frequent words are: {most_frequent}'\n",
    "\n",
    "\n",
    "#Let's get the most frequent from the text file\n",
    "most_frequent(txt,10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHT DATA\n",
      "Standard Deviation: 1.06\n",
      "Minimum: 3.0\n",
      "Maximum: 7.5\n",
      "Median: 4.5\n",
      "Mean: 4.71\n"
     ]
    }
   ],
   "source": [
    "# Read the cats API and find:\n",
    "# the min, max, mean, median, standard deviation of cats' weight in metric units.\n",
    "\n",
    "import requests\n",
    "import numpy\n",
    "cat_pi = requests.get('https://api.thecatapi.com/v1/breeds')\n",
    "data = cat_pi.json()\n",
    "\n",
    "\n",
    "def metric_weights(data):\n",
    "    metric_weights = []\n",
    "\n",
    "    for breed in data:\n",
    "        metric_weights.append(\n",
    "            (int(breed['weight']['metric'].split()[0]) + int(breed['weight']['metric'].split()[-1])) / 2)\n",
    "\n",
    "    median = numpy.median(metric_weights)\n",
    "    mean = numpy.mean(metric_weights)\n",
    "    min_metric = min(metric_weights)\n",
    "    max_metric = max(metric_weights)\n",
    "    sd = numpy.std(metric_weights)\n",
    "    print(\"WEIGHT DATA\")\n",
    "    print(\"Standard Deviation: %0.2f\" % sd)\n",
    "    print(\"Minimum:\", min_metric)\n",
    "    print(\"Maximum:\", max_metric)\n",
    "    print(\"Median:\", median)\n",
    "    print(\"Mean: %0.2f\" % mean)\n",
    "metric_weights(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 2ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIFESPAN DATA\n",
      "Standard Deviation: 1.57\n",
      "Minimum: 10.5\n",
      "Maximum: 19.0\n",
      "Median: 13.5\n",
      "Mean: 13.75\n"
     ]
    }
   ],
   "source": [
    "# the min, max, mean, median, standard deviation of cats' lifespan in years.\n",
    "\n",
    "import requests\n",
    "import numpy\n",
    "def life_spans(data):\n",
    "    lifespans = []\n",
    "    for breed in data:\n",
    "        lifespans.append((int(breed['life_span'].split()[0]) + int(breed['life_span'].split()[-1])) / 2)\n",
    "\n",
    "    median = numpy.median(lifespans)\n",
    "    mean = numpy.mean(lifespans)\n",
    "    min_span = min(lifespans)\n",
    "    max_span = max(lifespans)\n",
    "    sd = numpy.std(lifespans)\n",
    "    print(\"LIFESPAN DATA\")\n",
    "    print(\"Standard Deviation: %0.2f\" % sd)\n",
    "    print(\"Minimum:\", min_span)\n",
    "    print(\"Maximum:\", max_span)\n",
    "    print(\"Median:\", median)\n",
    "    print(\"Mean: %0.2f\" % mean)\n",
    "life_spans(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the countries API and find\n",
    "# the 10 largest countries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6c3004ce060ded9e0c076ab48db6d8cf68670c15173d9367e97ef3f186c9ed1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
